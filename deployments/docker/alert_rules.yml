groups:
  - name: meets-match-application
    rules:
      - alert: HighErrorRate
        expr: (
          sum(rate(http_requests_total{status=~"5.."}[5m])) by (service) /
          sum(rate(http_requests_total[5m])) by (service)
        ) > 0.05
        for: 2m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
        annotations:
          summary: "High error rate detected for {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} for service {{ $labels.service }}"

      - alert: HighResponseTime
        expr: (
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))
        ) > 2
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
        annotations:
          summary: "High response time for {{ $labels.service }}"
          description: "95th percentile response time is {{ $value }}s for service {{ $labels.service }}"

      - alert: ServiceDown
        expr: up{job=~"meets-match-.*"} == 0
        for: 1m
        labels:
          severity: critical
          service: "{{ $labels.job }}"
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} has been down for more than 1 minute"

      - alert: HighMemoryUsage
        expr: (
          process_resident_memory_bytes{job=~"meets-match-.*"} / 1024 / 1024 / 1024
        ) > 1
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.job }}"
        annotations:
          summary: "High memory usage for {{ $labels.job }}"
          description: "Memory usage is {{ $value | humanize }}GB for service {{ $labels.job }}"

      - alert: HighCPUUsage
        expr: (
          rate(process_cpu_seconds_total{job=~"meets-match-.*"}[5m]) * 100
        ) > 80
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.job }}"
        annotations:
          summary: "High CPU usage for {{ $labels.job }}"
          description: "CPU usage is {{ $value | humanizePercentage }} for service {{ $labels.job }}"

  - name: meets-match-database
    rules:
      - alert: DatabaseDown
        expr: up{job=~".*-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          database: "{{ $labels.job }}"
        annotations:
          summary: "Database {{ $labels.job }} is down"
          description: "Database exporter {{ $labels.job }} has been down for more than 1 minute"

      - alert: PostgreSQLHighConnections
        expr: (
          pg_stat_database_numbackends / pg_settings_max_connections * 100
        ) > 80
        for: 5m
        labels:
          severity: warning
          database: "postgresql"
        annotations:
          summary: "PostgreSQL high connection usage"
          description: "PostgreSQL connection usage is {{ $value | humanizePercentage }}"

      - alert: PostgreSQLSlowQueries
        expr: (
          rate(pg_stat_database_tup_returned[5m]) / rate(pg_stat_database_tup_fetched[5m])
        ) < 0.1
        for: 5m
        labels:
          severity: warning
          database: "postgresql"
        annotations:
          summary: "PostgreSQL slow queries detected"
          description: "Query efficiency is {{ $value | humanizePercentage }}"

      - alert: RedisHighMemoryUsage
        expr: (
          redis_memory_used_bytes / redis_memory_max_bytes * 100
        ) > 90
        for: 5m
        labels:
          severity: critical
          database: "redis"
        annotations:
          summary: "Redis high memory usage"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"

      - alert: RedisHighConnections
        expr: redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
          database: "redis"
        annotations:
          summary: "Redis high connection count"
          description: "Redis has {{ $value }} connected clients"

      - alert: RedisSlowLog
        expr: increase(redis_slowlog_length[5m]) > 10
        for: 2m
        labels:
          severity: warning
          database: "redis"
        annotations:
          summary: "Redis slow log entries increasing"
          description: "Redis slow log has increased by {{ $value }} entries in the last 5 minutes"

  - name: meets-match-infrastructure
    rules:
      - alert: HighDiskUsage
        expr: (
          (node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes * 100
        ) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      - alert: HighSystemLoad
        expr: node_load1 > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High system load on {{ $labels.instance }}"
          description: "System load is {{ $value }} on {{ $labels.instance }}"

      - alert: NginxDown
        expr: up{job="nginx-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          service: "nginx"
        annotations:
          summary: "Nginx is down"
          description: "Nginx has been down for more than 1 minute"

      - alert: NginxHighErrorRate
        expr: (
          sum(rate(nginx_http_requests_total{status=~"5.."}[5m])) /
          sum(rate(nginx_http_requests_total[5m]))
        ) > 0.05
        for: 2m
        labels:
          severity: warning
          service: "nginx"
        annotations:
          summary: "Nginx high error rate"
          description: "Nginx error rate is {{ $value | humanizePercentage }}"

  - name: meets-match-telegram-bot
    rules:
      - alert: TelegramBotHighErrorRate
        expr: (
          sum(rate(telegram_bot_errors_total[5m])) /
          sum(rate(telegram_bot_messages_total[5m]))
        ) > 0.1
        for: 2m
        labels:
          severity: warning
          service: "telegram-bot"
        annotations:
          summary: "Telegram bot high error rate"
          description: "Telegram bot error rate is {{ $value | humanizePercentage }}"

      - alert: TelegramBotSlowProcessing
        expr: (
          histogram_quantile(0.95, sum(rate(telegram_bot_processing_duration_seconds_bucket[5m])) by (le))
        ) > 5
        for: 5m
        labels:
          severity: warning
          service: "telegram-bot"
        annotations:
          summary: "Telegram bot slow message processing"
          description: "95th percentile processing time is {{ $value }}s for Telegram bot"

      - alert: TelegramBotQueueBacklog
        expr: telegram_bot_queue_size > 100
        for: 5m
        labels:
          severity: warning
          service: "telegram-bot"
        annotations:
          summary: "Telegram bot queue backlog"
          description: "Telegram bot has {{ $value }} messages in queue"

  - name: meets-match-signoz
    rules:
      - alert: SigNozDown
        expr: up{job=~"signoz-.*"} == 0
        for: 1m
        labels:
          severity: critical
          service: "signoz"
        annotations:
          summary: "SigNoz component {{ $labels.job }} is down"
          description: "SigNoz component {{ $labels.job }} has been down for more than 1 minute"

      - alert: ClickHouseDown
        expr: up{job="clickhouse"} == 0
        for: 1m
        labels:
          severity: critical
          service: "clickhouse"
        annotations:
          summary: "ClickHouse is down"
          description: "ClickHouse has been down for more than 1 minute"

      - alert: OtelCollectorHighDropRate
        expr: (
          sum(rate(otelcol_processor_dropped_spans_total[5m])) /
          sum(rate(otelcol_processor_accepted_spans_total[5m]))
        ) > 0.05
        for: 2m
        labels:
          severity: warning
          service: "otel-collector"
        annotations:
          summary: "OpenTelemetry Collector high drop rate"
          description: "OTel Collector span drop rate is {{ $value | humanizePercentage }}"